{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining using SAT-6 data\n",
    "\n",
    "#### First, read the files created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import fiona\n",
    "from classification import simplenet4 as net\n",
    "\n",
    "# Read files \n",
    "trainX = np.load('/scratch/slums/bl-slums/gt/trainSATX')\n",
    "trainY = np.load('/scratch/slums/bl-slums/gt/trainSATY')\n",
    "valX = np.load('/scratch/slums/bl-slums/gt/valSATX')\n",
    "valY = np.load('/scratch/slums/bl-slums/gt/valSATY')\n",
    "testX = np.load('/scratch/slums/bl-slums/gt/testSATX')\n",
    "testY = np.load('/scratch/slums/bl-slums/gt/testSATY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 32415  | total loss: \u001b[1m\u001b[32m0.04493\u001b[0m\u001b[0m | time: 494.787s\n",
      "| Adam | epoch: 032 | loss: 0.04493 - acc: 0.9827 -- iter: 259072/259197\n",
      "Training Step: 32416  | total loss: \u001b[1m\u001b[32m1.47061\u001b[0m\u001b[0m | time: 519.684s\n",
      "| Adam | epoch: 032 | loss: 1.47061 - acc: 0.9071 | val_loss: 0.02634 - val_acc: 0.9910 -- iter: 259197/259197\n",
      "--\n",
      "INFO:tensorflow:/scratch/slums/bl-slums/model/model--32416 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:/scratch/slums/bl-slums/model/best-model-9910 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:/scratch/slums/bl-slums/model/model--32416 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:/scratch/slums/bl-slums/model/final-model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# Prepare the training task\n",
    "import sys,os, datetime\n",
    "from tflearn.data_utils import shuffle\n",
    "# Get the model\n",
    "model = net.model\n",
    "\n",
    "\n",
    "# Load data\n",
    "trainX = trainX.reshape([-1, 28, 28, 4])\n",
    "print trainX.shape\n",
    "valX = valX.reshape([-1, 28, 28, 4])\n",
    "print valX.shape\n",
    "\n",
    "# Shuffle data\n",
    "trainX, trainY = shuffle(trainX, trainY)\n",
    "\n",
    "\n",
    "model.fit(trainX, trainY, n_epoch=32,validation_set=(valX, valY), show_metric=True, run_id=\"deep_nn2\", batch_size = 256,snapshot_step=100000)\n",
    "model.save(\"/scratch/slums/bl-slums/model/final-model.tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81000, 28, 28, 4)\n",
      "[[ 0.97065159  0.98649752  0.99703913  0.97642109  0.9821256   1.        ]]\n",
      "0.990950617284\n"
     ]
    }
   ],
   "source": [
    "#predict for the model\n",
    "testX = np.asarray(testX)\n",
    "predicted_all = []\n",
    "actual_all = []\n",
    "correct = np.zeros((1,6))\n",
    "total = np.zeros((1,6))\n",
    "testX = np.asarray(testX)\n",
    "print testX.shape\n",
    "for i in range(testX.shape[0]):\n",
    "    current = testX[i]\n",
    "    current = current.reshape(-1,28,28,4)\n",
    "    result = model.predict(current)[0] # Predict\n",
    "    #print result\n",
    "    prediction = np.argmax(result) # The index represents the number predicted in this case\n",
    "    actual = np.argmax(testY[i])\n",
    "    #print result, testY[i]\n",
    "    if actual == prediction:\n",
    "        correct[0,actual]+=1\n",
    "    total[0,actual]+=1\n",
    "    #print(\"Prediction: {}, Actual: {}\".format(prediction,actual))\n",
    "    predicted_all.append(prediction+1)\n",
    "    actual_all.append(actual+1)\n",
    "# Generate accuracy stats\n",
    "accuracies = np.zeros((1,6))\n",
    "for k in range(6):\n",
    "     accuracies[0,k] = correct[0,k]/total[0,k]\n",
    "print accuracies\n",
    "print np.sum(correct)/np.sum(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(actual_all, predicted_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
